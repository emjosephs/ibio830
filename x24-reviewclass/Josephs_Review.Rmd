---
title: "Model building: deterministic functions"
output:
  html_document: default
  pdf_document: default
language: R
---

<!-- To render the assignment in Rmarkdown, enter the command below in the R console -->
<!-- rmarkdown::render("Class_23_Assignment.Rmd") -->

#### Due by the beginning of next class (12/2)

Save this Rmarkdown script as a file named “YourlastName_Assignment23.Rmd”. 
For each question, fill out the answer and, where requested, 
provide the relevant R code using "```" and the echo = TRUE argument.

When finished, Knit your script together into an html report, 
saved as “YourlastName_Assignment23.html” and upload the 
resulting file to the D2L site (in today’s class folder) to turn it in. 
It is due before our next class.

\

**This homework assignment will test your abilities to:**

1. think critically and creatively about model building

2. extrapolate from in-class exercises to simulate data and do inference with a linear model

3. extrapolate from previous lecture materials to apply Bayes' Theorem to inference with a linear model

4. hone your R skillz: function building, data visualization, etc.

\

### Question 1

Explain in your own words what each of the terms in the equation below is, 
as well as what the equation as a whole is describing.

$Y_i \sim \mathcal{N}(\mu_i=f(x_i),\sigma)$

Y is the response variable for the ith individual/thing

mu is the mean value of the response variabl given the predictor variable for the ith individual/thing

sigma is the standard deviation of the normal distribution of mu

The entire equation is telling us for each individual, the expected value of the response variable is a function of the predictor variable with normally distributed error.


### Question 2

1. Simulate a dataset of 1000 datapoints using a linear model 
with Gaussian-distributed errors with the following parameter values:

	- intercept = 12
	- sigma = 10
	- predictor effect size = -1.3

You may generate values of the predictor variable however you wish.

```{r}

myPredictors = runif(1000, min=0, max=50)
myIntercept=12
mySigma=10
myBeta= -1.3

myResponse = sapply(myPredictors, function(i){
  myMean = mean=myIntercept + i*myBeta ## get the mean
  responseI = rnorm(n=1, myMean, sd=mySigma) ## add in the normally distributed error
  return(responseI)
 })



```


### Question 3

Use the function lm() to infer the parameter values used 
to simulate these data.  Confirm your results by overlaying the 
MLE parameter estimates on the real data (slope and intercept).

```{r}

myl=lm(myResponse~myPredictors)
summary(myl)


plot(myPredictors, myResponse, bty="n", col = "gray")
abline(myl, col = "magenta", lwd=2)
abline(a=myIntercept, b=myBeta, col = "darkgreen", lwd=2)


```


### Question 4

Calculate the numerator of Bayes' Theorem 
for the dataset you simulated in Question 1 
assuming the following:

* intercept ($\alpha$) = 13
* effect size ($\beta$) = -1
* standard deviation ($\sigma$) = 11
* $\alpha \sim \mathcal{N}(0,1)$
* $\beta \sim \mathcal{N}(0,1)$
* $\sigma \sim \text{Exp}(1)$

[hint: don't forget about the log product rule]

```{r}

## we want P(D|theta) x P(theta) = P(D|Alpha, Beta, Sigma) x p(Alpha, beta, sigma)
      
## calculate a log likelood for a given set of parameters and data
ln.L <- function(myY,myX,myAlpha,myBeta,mySigma){
    return(sum(
        dnorm(myY,mean=myAlpha + myBeta * myX,sd=mySigma,log=TRUE)
    ))
}

## this is the first part
firstPart = ln.L(myY = myResponse, myX = myPredictors, myAlpha = 13, myBeta = -1, mySigma=11)

## this is the second part
secondPart = dnorm(13, mean=0, sd=1, log=T) + dnorm(-1, mean=0, sd=1, log=T) + dexp(11, rate=1, log=T)

firstPart*secondPart

```

