---
title: "Intro to Bayes: Assignment"
output:
  html_document: default
language: R
---

<!-- To render the assignment in Rmarkdown, enter the command below in the R console -->
<!-- rmarkdown::render("Class_20_Assignment.Rmd") -->


#### Due by 11:59pm on 11/13

Save this Rmarkdown script as a file named “YourlastName_Assignment20.Rmd”. 
For each question, fill out the answer and, where requested, 
provide the relevant R code using "```" and the echo = TRUE argument.

When finished, Knit your script together into an html report, 
saved as “YourlastName_Assignment20.html” and upload the 
resulting file to the D2L site (in today’s class folder) to turn it in. 
It is due before our next class.

\

**This homework assignment will test your abilities to:**

1. think critically and creatively about Bayesian statistics

2. extrapolate from in-class exercises to apply Bayesian inference to arbitrary distributions

3. use your R skillz: function building, data visualization, etc.

\

### Question 1

Imagine that you are an alien who is new to earth. You are given a penny, which you flip 10 times. You get heads 5 times.

a) What is the likelihood of getting 4 flips out of 10 heads if the underlying probability of heads is 0.5?
```{r}
dbinom(x=4,size=10,prob=0.5)
```

b) What is the probability that p(heads) = 0.5 given that you've flipped the coin 10 times and gotten 4 heads?
(Hint: use Bayes theorem!)
(Hint 2: As an alien, you think the prior probability of p=0.5 is fairly low, like 0.0001)
(Hint 3: p(data) will be one, since it's the sum of the probabilities of getting this data across all possible parameters)
```{r}
p.posterior = (dbinom(x=4,size=10,prob=0.5)*0.0001)
p.posterior
```

c) A friendly human tells you that most coins are fair. This causes you, the alien, to change your mind about the prior probability that
p(heads) = 0.5. You now think that the prior is 0.75. Recalculate the posterior probability given this new prior.

```{r}
p.posterior = (dbinom(x=4,size=10,prob=0.5)*0.75)
p.posterior
```

d) Explain briefly, in words, how the prior probability affects the posterior probability.

Having a prior expectation of fairness increased the posterior probability that the coin is fair and p=0.5

### Question 3

The code below is what was used in the lecture to calculate the posterior probability of getting 4 heads out of 10 coin flips 
across a range of potential probabilities of heads and the prior that we are aliens and don't know how coins work.
```{r, echo=T}
## this function calculates the log likelihood of the flip data
	lnL.foo <- function(flips,n,p){
		lnL <- sum(dbinom(flips,n,p,log=TRUE))
		return(lnL)
	}

#set the number of flips that give heads
flips1 <- 4

#make a grid of possible values of p
p.seq <- seq(1e-4,1-1e-4,length.out=500)

#calculate-likelihoods for the grid of p
	lnL1 <- sapply(p.seq,function(p){lnL.foo(flips1,n=10,p=p)})
	
#set a prior as if we are aliens
	pr <- dunif(p.seq,log=TRUE)

#calculate the log posterior probability
	ps <- lnL1 + pr
	
#plot the data
plot(p.seq,ps,ylim=c(-15,5),type='l',col=2,lwd=2,ylab="log posterior probability",xlab="values of p",
	 	main=sprintf("10 coin flips (%s heads)",flips1))
	
```

a) Look over the code abve. When the code calculates the posterior probability (ps1), it adds the log likelihood (lnL1) to the prior (pr1).
How is this different from the standard Bayes theorem we learned about in class? Why is it different?


b) Edit the code from part a to calculate and plot the log posterior probability of flipping a coin 100 times and getting heads 40 times. 
How does the posterior probability differ from the probabilities in part a? Why?
	
```{r, echo=T}
#editted code goes here.	
	
```
