---
title: "Lecture 19 - Introduction to Likelihood"
author: "Emily Josephs"
date: "November 12 2024"
output:
  ioslides_presentation:
    transition: 0.001
    bigger: true
    incremental: true
---
<!-- To render the lecture in Rmarkdown, enter the command below in the R console -->
<!-- rmarkdown::render("lecture19.Rmd") -->

```{r,echo=FALSE}
	#set any global options
	options(digits=3)
	set.seed(123)
```


## Introduction to Likelihood

Team discussion problem: Someone hands us a coin and we flip it 100 times and get 46 heads. 

Is the coin fair? (probability of heads = 50%)

What is the _most likely_ probability of flipping heads for this coin?


## Introduction to Likelihood {.build}

The **likelihood** of the data is the probability of the 
data as a function of some unknown parameters.

<div class="centered">
\
\
\
\
Likelihood is probability...in reverse!
</div>

## Probability vs. Statistics{.build}

- In probability, we think about some stochastic process, 
 and figure out ways to calculate the probability of possible outcomes
 		
- In statistics, we start with some observed outcomes, 
 and try to figure out the underlying process.


##  Probability vs. Statistics{.build}

probability says: given that the coin is fair, what's the probability of getting 46 heads out of 100 flips?

statistics says: given some flip data, can we figure out the fairness of the coin?

##  Probability vs. Statistics{.build}

In calculating probabilities, we consider a single parameter value 
and describe the probabilities of all possible outcomes of a 
process parameterized by that value.

In calculating likelihoods, we consider a single outcome (or set of outcomes) and many possible parameter values that could best explain it.

In formulating the problem this way, 
we are treating the observed data ($k=46$) as a _known_, 
and treating $p$ as an unknown **_parameter_** of the model.

## Parametric Statistics {.build}

In parametric inference, 
we treat observed data as draws from 
an underlying process or **_population_**, 
and we try to learn about that population from our sample.

## Parameters {.build}

A **_statistical parameter_** is a value that tells you something 
about a **_population_**.

- e.g., the _true_ mean height of students in our class, 
or the _true_ frequency of a genetic variant in a species.
\

We rarely get to know the truth 

(by, e.g., censusing everyone in a population, 
or repeating a random process an infinite number of times) 

but we can take _samples_ to try to learn about --  
or **_estimate_** -- parameters.  

## Back to the coin flip problem {.build}

We flip a coin 100 times and get 46 heads.
```{r, echo=FALSE}
n.heads <- 46
```

<div class="centered">
$\large X \sim B(n,p)$
</div>

<div class="centered">
$\large 46 \sim B(n=100,p=??)$
</div>

<div class="centered">
$\large p(observation) = p(X=46 \mid p, n=100)$
</div>

\
\

## Calculating likelihood with math? {.build}

What is the probabiliy that the coin is fair?

<div class="centered">
$\large X \sim B(n=100,p=0.5)$\
\

$\large 
\begin{aligned}
p(X=46) &= {{n}\choose{k}}p^k (1-p)^{n-k} \\ 
\\
&= {{100}\choose{46}}(0.5)^{46} (0.5)^{54}
\end{aligned}$
</div>

```{r, eval=T}
choose(n=100,k=46)*(0.5^46)*(0.5^54)
```


  
## Calculating likelihood with dbinom! {.build}

```{r, eval=T}
dbinom(x=46, p=0.5, size=100)
```


## Calculating likelihood for one value of p
```{r,echo=T}
	p <- 0.1
	plot(p,dbinom(x=n.heads,size=100,p=p),xlim=c(0,1),ylim=c(0,0.1), xlab="probability of heads (p)",ylab="p(data)", pch=20,col=2)
```

## Calculating likelihood for 2 values of p
```{r,echo=T}
	p <- c(0.1,0.5)
	plot(p,dbinom(x=n.heads,size=100,p=p), xlim=c(0,1),ylim=c(0,0.1),, xlab="probability of heads (p)",ylab="p(data)", pch=20,col=2)
```

## Calculating likelihood for many ps

```{r,echo=T}
	p <- runif(15,min=0,max=1)
	plot(p,dbinom(x=n.heads,size=100,p=p), xlim=c(0,1),ylim=c(0,0.1), xlab="probability of heads (p)",ylab="p(data)", pch=20,col=2)
```

## Calculating likelihood with a grid search
```{r,echo=T}
	p <- seq(0,1,length.out=1e3) 
	plot(p,dbinom(x=n.heads,size=100,p=p), xlim=c(0,1),ylim=c(0,0.1), xlab="probability of heads (p)",ylab="p(data)", pch=20,col=2)
	arrows(x0=0.46,x1=0.46,y0=0.092,y1=0.085,length=0.1)
```

## Maximum likelihood {.build}

_Maximum likelihood (ML) inference_ is a method for estimating the values 
of the parameters of a statistical model that maximize the likelihood 
of the observed data.

The _maximum likelihood estimate_ (MLE) is the parameter value
(or, if there are multiple parameters, the vector of parameter values) 
that maximize the likelihood of the data.

The MLE is our best guess at the true value of the unknown 
**_population_** (or process) parameter.

## How do we find the MLE? {.build}

Working in your breakout groups, 
do a grid search to find the MLE of $p$ 
when you observe $k=37$ heads out of $n=57$ flips.

You will need: 
```{r,eval=FALSE}
help(seq)
help(dbinom)
help(which.max)
```

## Grid search to find the MLE

```{r}
p <- seq(0,1,length.out=1000)
likelihoods <- dbinom(x=37,size=57,p=p)
p[which.max(likelihoods)]
```

## Grid search to find the MLE

```{r,echo=T}
plot(p,likelihoods,xlab="p(heads)",ylab="p(data)")
	abline(v=p[which.max(likelihoods)],col=2)
	legend(x="topleft",lty=1,col=2,legend="MLE")
```

## Using optimize to find the MLE {.build}

```{r,echo=T}
# function to estimate the p(k=46|n=100,p) for a given p
	l.binom <- function(p){
		binom.prob <- dbinom(x=46,size=100,prob=p)
		return(binom.prob)
	}
# use optimize() to get the mle
	mle <- optimize(f = l.binom,lower=0,upper=1,maximum=TRUE)
	mle
```

Note, maximum is the parameter, objective is the likelihood of the data given the parameter

## Now hang on... {.build}

Calculating the probability of some possible outcome makes sense...

But calculating the probability of something that has _already happened_ seems **bananas**!


## Maximum Likelihood - not bananas* {.build}

But, that's not really what we're doing - 

We're calculating the probability of the observed data as a draw from some distribution,
**_given_** the values of the parameters of that distribution.

Our goal is to use these probabilities to estimate the distribution parameters.


## Maximum Likelihood: recap

**_Maximum likelihood (ML) inference_** is a method for estimating the values 
of the parameters of a statistical model that maximize the likelihood 
of the observed data.

The **_maximum likelihood estimate_** (**MLE**) is the parameter value
(or, if there are multiple parameters, the vector of parameter values) 
that maximize the likelihood of the data.

There are efficient algorithms that estimate the parameter values 
that maximize the likelihood of the data, and we can use them to do 
inference!

## Multiple observations {.build}

So we know how to take the likelihood of a single observation.

What about if we have _multiple observations_?

## Multiple observations {.build}

Assuming the observations are independent and identically distributed (i.i.d.), 
their probabilities can be computed as a single **shared event**.


$\large p(A \; \cap \; B) = p(A) \times p(B)$



$\large
\begin{aligned}
p(\vec{X}=\{46,47,48\}) =& \; p(X_1=46) \\
&\times p(X_2=47) \\ 
&\times p(X_3=48)
\end{aligned}$


## Multiple observations {.build}

$p(X={46,47,48}) = p(X=46) \times p(X=47) \times p(X=48)$
```{r,echo=FALSE}
n.heads <- c(46,47,48)
```

If _p_(heads)=0.5 and _n_=100, how would we use code to
estimate $p(X={46,47,48})$? 

```{r, echo=T, eval=T}
prod(dbinom(x=c(46,47,48), size=100, prob=0.5))
```

What if we want to know the likelihood of 10 observations?
```{r}
myObservations = rbinom(10,size=100,prob=0.5)
prod(dbinom(x=myObservations,size=100,prob=0.5))
```

## Multiple observations {.build}

What about 50 observations?

```{r}
myObservations = rbinom(50,size=100,prob=0.5)
prod(dbinom(x=myObservations,size=100,prob=0.5))
```

```{r}
myObservations = rbinom(500,size=100,prob=0.5)
prod(dbinom(x=myObservations,size=100,prob=0.5))
```

## What went wrong?

<div class="centered">
![](figs/comp_on_fire.jpg)
</div>

## What went wrong? {.build}

### Underflow

The product of small numbers are smaller numbers.

Very very very small numbers cannot be represented in your computer's memory.

## So how can we calculate the likelihood of multiple observations? {.build}


## Logs!

Recall that:

$$\text{log}(A \times B \times C) = \text{log}(A) + \text{log}(B) + \text{log}(C)$$
\
and more generally:

$\text{log}\left(\prod\limits_{i=1}^n X_i \right) = \sum\limits_{i=1}^n \text{log}(X_i)$

## Logs!

The log is *monotonic*.

If $X > Y$ then $\text{log}(X) > \text{log}(Y)$

```{r, echo=F}
plot((1:100)/100, log((1:100)/100), bty="n", xlab = "X", ylab = "log(X)")
```

## Log Likelihood

```{r,echo=T}
#simulate data
heads <- rbinom(10,size=100,prob=0.5)

#grid search input
probs <- seq(0,1,length.out=50)

#calculate likelihood across the grid
likelihood <- sapply(probs,function(p)
  {prod(dbinom(heads,100,p))})

#calculate log likelihood across the grid
log.likelihood <- sapply(probs,function(p)
  {sum(dbinom(heads,100,p,log=TRUE))})
```

## Log Likelihood
```{r,echo=T}
par(mfrow=c(1,2))
	plot(probs,likelihood,main="likelihood",pch=20,xlab="p(heads)",ylab="likelihood")
	plot(probs,log.likelihood,main="log likelihood",pch=20,xlab="p(heads)",ylab="log likelihood")
```

## Likelihood and number of observations {.build}

What do you think will happen to the likelihood as 
we increase the number of observations?

Poll:\

A - the slopes of the likelihood peak will become steeper \
B - the slopes of the likelihood peak will become less steep \
C - nothing will change \


```{r,echo=FALSE}
n.heads <- rbinom(n=1e4,size=100,prob=0.7)
probs <- seq(0.001,0.999,length.out=50)
lnL1 <- sapply(probs,function(p){
			sum(dbinom(n.heads[1:100],100,p,log=TRUE))})

lnL2 <- sapply(probs,function(p){
			sum(dbinom(n.heads[1:1000],100,p,log=TRUE))})

lnL3 <- sapply(probs,function(p){
			sum(dbinom(n.heads[1:10000],100,p,log=TRUE))})
```

## Likelihood and number of observations

```{r,echo=FALSE,fig.width=7,fig.height=5.5}
plot(probs,lnL1,ylab="log likelihood",ylim=c(-2e5,5e4),type='n')
	abline(v=0.7,col=1,lty=2)
legend(x="topleft",lty=1,lwd=3,
		col=c("blue","purple","red"),
		legend=c("n=1e2","n=1e3","n=1e4"))
```

## Likelihood and number of observations

```{r,echo=FALSE,fig.width=7,fig.height=5.5}
plot(probs,lnL1,ylab="log likelihood",ylim=c(-2e5,5e4),type='n')
	abline(v=0.7,col=1,lty=2)
	lines(probs,lnL1,col="blue")
	lines(probs,lnL2,col="purple")
	lines(probs,lnL3,col="red")
legend(x="topleft",lty=1,lwd=3,
		col=c("blue","purple","red"),
		legend=c("n=1e2","n=1e3","n=1e4"))
```

## Likelihood recap {.build}

We can calculate the likelihood of observed data given values of the parameters of a distribution

**_Maximum likelihood (ML) inference_** is a method for estimating the values 
of the parameters of a statistical model that maximize the likelihood 
of the observed data.

The **_maximum likelihood estimate_** (MLE) is the parameter value
(or, if there are multiple parameters, the vector of parameter values) 
that maximize the likelihood of the data.

When we have multiple observations, we calculate their likelihood as the 
product of their individual likelihoods, or (better) the sum of their **_log likelihoods_**.

## The Law of Large Numbers {.build}

$X \sim B(n=100, p=0.3)$

```{r,echo=FALSE}
par(mfrow=c(1,2))
	x1 <- rbinom(5,size=100,prob=0.3)
	x2 <- rbinom(5e6,size=100,prob=0.3)
		hist(x1,col="slateblue",main=sprintf("5 samples\nE[X] - mean(X) = %s",signif(30-mean(x1),3)),xlim=c(0,100))
			abline(v=mean(x1),col="red",lwd=2)
			abline(v=100*0.3,col="green",lwd=2)
		hist(x2,col="slateblue",main=sprintf("5e6 samples\nE[X] - mean(X) = %s",signif(30-mean(x2),3)),xlim=c(0,100))
			abline(v=mean(x2),col="red",lwd=2)
			abline(v=100*0.3,col="green",lwd=2)
		legend(x="topright",lty=1,col=c("red","green"),legend=c("sample mean","expectation"),cex=0.7)
```

## The Law of Large Numbers {.build}

As the number of samples increases, the sample average converges on the 
expected value.

Discovered by our friend Jacob/James/Jaccques Bernoulli!

## Visualizing the Law of Large Numbers

```{r,echo=TRUE,eval=F}
	draw.running.mean <- function(r,n){
		replicate(r,{
			x1 <- rnorm(n,mean=0,sd=0.5)
			x1.mean <- cumsum(x1)/(1:n)
			lines(1:n,x1.mean,col=adjustcolor(1,0.2))
		})
		return(invisible(0))
	}
	n <- 1e2
	plot(0,xlim=c(0,n),ylim=c(-0.5,0.5),type='n',
			xlab="number of draws from a normal",
			ylab="running mean")
		draw.running.mean(r = 1,n = 100)
		abline(h=0,col=2)
```

## Visualizing the Law of Large Numbers

```{r,echo=F,eval=T, cache=T}
	draw.running.mean <- function(r,n){
		replicate(r,{
			x1 <- rnorm(n,mean=0,sd=0.5)
			x1.mean <- cumsum(x1)/(1:n)
			lines(1:n,x1.mean,col=adjustcolor(1,0.2))
		})
		return(invisible(0))
	}
	n <- 1e2
	plot(0,xlim=c(0,n),ylim=c(-0.5,0.5),type='n',
			xlab="number of draws from a normal",
			ylab="running mean")
		draw.running.mean(r = 1,n = 100)
		abline(h=0,col=2)
```


## Visualizing the Law of Large Numbers

```{r,echo=FALSE,cache=TRUE}
	n <- 1e2
	plot(0,xlim=c(0,n),ylim=c(-0.5,0.5),type='n',
			xlab="number of draws from a normal",
			ylab="running mean")
		draw.running.mean(r = 1,n = 100)
		abline(h=0,col=2)
```


## Visualizing the Law of Large Numbers

```{r,echo=FALSE,cache=TRUE}

	n <- 1e2
	plot(0,xlim=c(0,n),ylim=c(-0.5,0.5),type='n',
			xlab="number of draws from a normal",
			ylab="running mean")
		draw.running.mean(r = 100,n = n)
		abline(h=0,col=2)
```


## The law of large numbers in population genetics

This is why genetic drift is stronger in small populations!!

https://cjbattey.shinyapps.io/driftR/


## Up next

Building linear models with maximum likelihood